{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção do modelo vgg16. As feed forward layers foram retreinadas para o probelma das assinaturas enquanto os pesos das camadas anteriores foram reaproveitados de um modelo já treinado para identificação de objetos (os pesos estão no arquivo weights.npz).\n",
    "Nenhuma entrada é necessária, basta executar.\n",
    "\n",
    "Obs: os trechos comentados são para algumas alterações futuras, não possuem nenhuma importância no momento e podem ser ignorados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# =============================================================================\n",
    "num_classes = 2\n",
    "image_size = 224\n",
    "root = '.'\n",
    "num_channels = 3\n",
    "fc_input_size = 25088 # nao muito interessante inicializar o valor na mao\n",
    "# =============================================================================\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "x_valid = np.load('x_valid.npy')\n",
    "y_valid = np.load('y_valid.npy')\n",
    "\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "# =============================================================================\n",
    "train_batches_x = np.reshape(x_train, (18, -1, image_size, image_size, 3))\n",
    "train_batches_y = np.reshape(y_train, (18, -1, 2))\n",
    "\n",
    "valid_batches_x = np.reshape(x_valid, (5, -1, image_size, image_size, 3))\n",
    "valid_batches_y = np.reshape(y_valid, (5, -1, 2))\n",
    "\n",
    "x_test = x_test[:x_test.shape[0] - 1]\n",
    "y_test = y_test[:y_test.shape[0] - 1]\n",
    "test_batches_x = np.reshape(x_test, (4, -1, image_size, image_size, 3))\n",
    "test_batches_y = np.reshape(y_test, (4, -1, 2))\n",
    "\n",
    "print('train_batches_x:', train_batches_x.shape)\n",
    "print('train_batches_y: ', train_batches_y.shape)\n",
    "# =============================================================================\n",
    "weight_file = 'vgg16_weights.npz'\n",
    "class_names = ['forgery', 'genuine']\n",
    "\n",
    "weights = np.load(weight_file)\n",
    "keys = sorted(weights.keys())\n",
    "for i, k in enumerate(keys):\n",
    "    print (i, k, np.shape(weights[k]))\n",
    "# =============================================================================\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    # keep probability for dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # inputs\n",
    "    data = tf.placeholder(tf.float32, shape=(None, image_size, image_size,\n",
    "                                             num_channels))\n",
    "    labels = tf.placeholder(tf.float32, shape=(None, num_classes))\n",
    "    \n",
    "    # conv1_1 parameters\n",
    "    conv1_1_weights = tf.Variable(weights['conv1_1_W'], trainable=False)\n",
    "    conv1_1_biases = tf.Variable(weights['conv1_1_b'], trainable=False)\n",
    "    \n",
    "    # conv1_2 parameters\n",
    "    conv1_2_weights = tf.Variable(weights['conv1_2_W'], trainable=False)\n",
    "    conv1_2_biases = tf.Variable(weights['conv1_2_b'], trainable=False)\n",
    "    \n",
    "    # conv2_1 parameters\n",
    "    conv2_1_weights = tf.Variable(weights['conv2_1_W'], trainable=False)\n",
    "    conv2_1_biases = tf.Variable(weights['conv2_1_b'], trainable=False)\n",
    "    \n",
    "    # conv2_2 parameters\n",
    "    conv2_2_weights = tf.Variable(weights['conv2_2_W'], trainable=False)\n",
    "    conv2_2_biases = tf.Variable(weights['conv2_2_b'], trainable=False)\n",
    "    \n",
    "    # conv3_1 parameters\n",
    "    conv3_1_weights = tf.Variable(weights['conv3_1_W'], trainable=False)\n",
    "    conv3_1_biases = tf.Variable(weights['conv3_1_b'], trainable=False)\n",
    "    \n",
    "    # conv3_2 parameters\n",
    "    conv3_2_weights = tf.Variable(weights['conv3_2_W'], trainable=False)\n",
    "    conv3_2_biases = tf.Variable(weights['conv3_2_b'], trainable=False)\n",
    "    \n",
    "    # conv3_3 parameters\n",
    "    conv3_3_weights = tf.Variable(weights['conv3_3_W'], trainable=False)\n",
    "    conv3_3_biases = tf.Variable(weights['conv3_3_b'], trainable=False)\n",
    "    \n",
    "    # conv4_1 parameters\n",
    "    conv4_1_weights = tf.Variable(weights['conv4_1_W'], trainable=False)\n",
    "    conv4_1_biases = tf.Variable(weights['conv4_1_b'], trainable=False)\n",
    "    \n",
    "    # conv4_2 parameters\n",
    "    conv4_2_weights = tf.Variable(weights['conv4_2_W'], trainable=False)\n",
    "    conv4_2_biases = tf.Variable(weights['conv4_2_b'], trainable=False)\n",
    "    \n",
    "    # conv4_3 parameters\n",
    "    conv4_3_weights = tf.Variable(weights['conv4_3_W'], trainable=False)\n",
    "    conv4_3_biases = tf.Variable(weights['conv4_3_b'], trainable=False)\n",
    "    \n",
    "    # conv5_1 parameters\n",
    "    conv5_1_weights = tf.Variable(weights['conv5_1_W'], trainable=False)\n",
    "    conv5_1_biases = tf.Variable(weights['conv5_1_b'], trainable=False)\n",
    "    \n",
    "    # conv5_2 parameters\n",
    "    conv5_2_weights = tf.Variable(weights['conv5_2_W'], trainable=False)\n",
    "    conv5_2_biases = tf.Variable(weights['conv5_2_b'], trainable=False)\n",
    "    \n",
    "    # conv5_3 parameters\n",
    "    conv5_3_weights = tf.Variable(weights['conv5_3_W'], trainable=False)\n",
    "    conv5_3_biases = tf.Variable(weights['conv5_3_b'], trainable=False)\n",
    "    \n",
    "    # fc1 parameters\n",
    "    fc1_weights = tf.Variable(weights['fc6_W'], trainable=True)\n",
    "    fc1_biases = tf.Variable(weights['fc6_b'], trainable=True)\n",
    "    \n",
    "    # fc2 parameters\n",
    "    fc2_weights = tf.Variable(weights['fc7_W'], trainable=True)\n",
    "    fc2_biases = tf.Variable(weights['fc7_b'], trainable=True)\n",
    "    \n",
    "    # fc3 parameters\n",
    "    fc3_weights = tf.Variable(weights['fc8_W'], trainable=True)\n",
    "    fc3_biases = tf.Variable(weights['fc8_b'], trainable=True)\n",
    "    \n",
    "    # fc4 parameters (added for the signature problem)\n",
    "    fc4_weights = tf.Variable(tf.truncated_normal([1000, 2], stddev=0.1),\n",
    "                              trainable=True)\n",
    "    fc4_biases = tf.Variable(tf.zeros([2]), trainable=True)\n",
    "    \n",
    "    # build the model\n",
    "    def model(data):\n",
    "        # convolutional layers\n",
    "        conv1_1 = tf.nn.conv2d(data, conv1_1_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv1_1 = tf.nn.bias_add(conv1_1, conv1_1_biases)\n",
    "        conv1_1 = tf.nn.relu(conv1_1)\n",
    "        \n",
    "        conv1_2 = tf.nn.conv2d(conv1_1, conv1_2_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv1_2 = tf.nn.bias_add(conv1_2, conv1_2_biases)\n",
    "        conv1_2 = tf.nn.relu(conv1_2)\n",
    "        \n",
    "        pool1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME') # tentar remover o padding\n",
    "        \n",
    "        conv2_1 = tf.nn.conv2d(pool1, conv2_1_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv2_1 = tf.nn.bias_add(conv2_1, conv2_1_biases)\n",
    "        conv2_1 = tf.nn.relu(conv2_1)\n",
    "        \n",
    "        conv2_2 = tf.nn.conv2d(conv2_1, conv2_2_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv2_2 = tf.nn.bias_add(conv2_2, conv2_2_biases)\n",
    "        conv2_2 = tf.nn.relu(conv2_2)\n",
    "        \n",
    "        pool2 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME') # tentar remover o padding\n",
    "        \n",
    "        conv3_1 = tf.nn.conv2d(pool2, conv3_1_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv3_1 = tf.nn.bias_add(conv3_1, conv3_1_biases)\n",
    "        conv3_1 = tf.nn.relu(conv3_1)\n",
    "        \n",
    "        conv3_2 = tf.nn.conv2d(conv3_1, conv3_2_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv3_2 = tf.nn.bias_add(conv3_2, conv3_2_biases)\n",
    "        conv3_2 = tf.nn.relu(conv3_2)\n",
    "        \n",
    "        conv3_3 = tf.nn.conv2d(conv3_2, conv3_3_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv3_3 = tf.nn.bias_add(conv3_3, conv3_3_biases)\n",
    "        conv3_3 = tf.nn.relu(conv3_3)\n",
    "        \n",
    "        pool3 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME') # tentar remover o padding\n",
    "        \n",
    "        conv4_1 = tf.nn.conv2d(pool3, conv4_1_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv4_1 = tf.nn.bias_add(conv4_1, conv4_1_biases)\n",
    "        conv4_1 = tf.nn.relu(conv4_1)\n",
    "        \n",
    "        conv4_2 = tf.nn.conv2d(conv4_1, conv4_2_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv4_2 = tf.nn.bias_add(conv4_2, conv4_2_biases)\n",
    "        conv4_2 = tf.nn.relu(conv4_2)\n",
    "        \n",
    "        conv4_3 = tf.nn.conv2d(conv4_2, conv4_3_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv4_3 = tf.nn.bias_add(conv4_3, conv4_3_biases)\n",
    "        conv4_3 = tf.nn.relu(conv4_3)\n",
    "        \n",
    "        pool4 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME') # tentar remover o padding\n",
    "        \n",
    "        conv5_1 = tf.nn.conv2d(pool4, conv5_1_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv5_1 = tf.nn.bias_add(conv5_1, conv5_1_biases)\n",
    "        conv5_1 = tf.nn.relu(conv5_1)\n",
    "        \n",
    "        conv5_2 = tf.nn.conv2d(conv5_1, conv5_2_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv5_2 = tf.nn.bias_add(conv5_2, conv5_2_biases)\n",
    "        conv5_2 = tf.nn.relu(conv5_2)\n",
    "        \n",
    "        conv5_3 = tf.nn.conv2d(conv5_2, conv5_3_weights, [1, 1, 1, 1],\n",
    "                               padding='SAME')\n",
    "        conv5_3 = tf.nn.bias_add(conv5_3, conv5_3_biases)\n",
    "        conv5_3 = tf.nn.relu(conv5_3)\n",
    "        \n",
    "        pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1], padding='SAME') # tentar remover o padding\n",
    "               \n",
    "        # fully connected layers\n",
    "        # fc1\n",
    "        pool5_flat = tf.reshape(pool5, [-1, fc_input_size])\n",
    "        fc1 = tf.matmul(pool5_flat, fc1_weights)\n",
    "        fc1 = tf.nn.bias_add(fc1, fc1_biases)\n",
    "        fc1= tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        \n",
    "        # fc2\n",
    "        fc2 = tf.matmul(fc1, fc2_weights)\n",
    "        fc2 = tf.nn.bias_add(fc2, fc2_biases)\n",
    "        fc2 = tf.nn.dropout(fc2, keep_prob=keep_prob)\n",
    "        fc2 = tf.nn.relu(fc2)\n",
    "        \n",
    "        # fc3\n",
    "        fc3 = tf.matmul(fc2, fc3_weights)\n",
    "        fc3 = tf.nn.bias_add(fc3, fc3_biases)\n",
    "        fc3 = tf.nn.dropout(fc3, keep_prob=keep_prob)\n",
    "        fc3 = tf.nn.relu(fc3)\n",
    "        \n",
    "        # fc4\n",
    "        fc4 = tf.matmul(fc3, fc4_weights)\n",
    "        fc4 = tf.nn.bias_add(fc4, fc4_biases)\n",
    "        return tf.nn.dropout(fc4, keep_prob=keep_prob)\n",
    "    \n",
    "    \n",
    "    # optimizing\n",
    "    beta = 0.002\n",
    "    logits = model(data)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    regularizer = tf.nn.l2_loss(conv1_1_weights) + tf.nn.l2_loss(conv1_2_weights) + tf.nn.l2_loss(conv2_1_weights) + tf.nn.l2_loss(conv2_2_weights) + tf.nn.l2_loss(conv3_1_weights) + tf.nn.l2_loss(conv3_2_weights) + tf.nn.l2_loss(conv3_3_weights) + tf.nn.l2_loss(conv4_1_weights) + tf.nn.l2_loss(conv4_2_weights) + tf.nn.l2_loss(conv4_3_weights) + tf.nn.l2_loss(conv5_1_weights) + tf.nn.l2_loss(conv5_2_weights) + tf.nn.l2_loss(conv5_3_weights) + tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc3_weights) + tf.nn.l2_loss(fc4_weights)# l2 regularization\n",
    "    \n",
    "    loss = tf.reduce_mean(loss + beta * regularizer)\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "    # prediction and accuracy\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "    model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# =============================================================================\n",
    "num_steps = 40\n",
    "validation_accuracies = list()\n",
    "train_losses = list()\n",
    "valid_losses = list()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        for batch in range(train_batches_x.shape[0]):\n",
    "            batch_data = train_batches_x[batch, :, :, :, :]\n",
    "            batch_label = train_batches_y[batch, :, :]\n",
    "            _, l, acc = session.run([optimizer, loss, model_accuracy], \n",
    "                                    feed_dict={data:batch_data, \n",
    "                                               labels:batch_label, \n",
    "                                               keep_prob:0.5})\n",
    "            if (batch % 3 == 0):\n",
    "                print('loss at epoch %d batch %d: %f' % (step, batch, l))\n",
    "                \n",
    "                acc = 0\n",
    "                for i in range(valid_batches_x.shape[0]):\n",
    "                    acc += session.run([model_accuracy], feed_dict={data:valid_batches_x[i],\n",
    "                                      labels:valid_batches_y[i], keep_prob:1})[0]\n",
    "                acc /= valid_batches_x.shape[0]\n",
    "                train_losses.append(l)\n",
    "                validation_accuracies.append(acc)\n",
    "                print('valid accuracy:', validation_accuracies[-1])\n",
    "                print()\n",
    "    # print('Test accuracy:', session.run([model_accuracy], feed_dict={data:x_test, labels:y_test}))\n",
    "    \n",
    "    test_predictions = list()\n",
    "    for test in test_batches_x:\n",
    "         temp = session.run([prediction], feed_dict={data:test, keep_prob:1})\n",
    "         test_predictions += temp\n",
    "\n",
    "np.save('train_losses.npy', train_losses)\n",
    "np.save('validation_accuracies.npy', validation_accuracies)\n",
    "np.save('test_predictions.npy', test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
